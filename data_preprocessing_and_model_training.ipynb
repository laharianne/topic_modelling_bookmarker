{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; \n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from utils import extract_text_from_url\n",
    "import json\n",
    "# to suppress warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating corpus\n",
    "variety_corpus_1 = []\n",
    "with open('/Users/laharianne/Desktop/Spring 2024/CS 510 Adv Info Retrieval/topic_modelling_bookmarker/Datasets/News_Category_Dataset_v3.json', 'r') as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Parse the JSON object in each line\n",
    "        data = json.loads(line)\n",
    "        # Extract the short description and append it to the list\n",
    "        short_description = data.get('short_description', '')\n",
    "        line_sentences = short_description.strip().split('. ')\n",
    "        variety_corpus_1.extend(line_sentences)\n",
    "        # variety_corpus_1.append(short_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'utf-8' codec can't decode byte 0xb8 in position 99: invalid start byte\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "variety_corpus_2 = []\n",
    "import os\n",
    "folder_path = '/Users/laharianne/Desktop/Spring 2024/CS 510 Adv Info Retrieval/topic_modelling_bookmarker/Datasets/SoUcorpus_selected'\n",
    "def get_files_in_folder(folder_path):\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "            file_names.append(file_name)\n",
    "    \n",
    "    # Return the list of file names\n",
    "    return file_names\n",
    "\n",
    "file_names = get_files_in_folder(folder_path)\n",
    "count = 0\n",
    "for filename in file_names:\n",
    "    try:\n",
    "        with open(os.path.join(folder_path, filename), 'r') as file:\n",
    "            # Iterate over each line in the file\n",
    "            for line in file:\n",
    "                # Split the line into sentences based on the full stop\n",
    "                line_sentences = line.strip().split('. ')\n",
    "                # Append each sentence to the list of sentences\n",
    "                variety_corpus_2.extend(line_sentences)\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        print(e)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['STORY', 'SECTION'], dtype='object')\n",
      "Index(['STORY'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "variety_corpus_3 = []\n",
    "files = ['/Users/laharianne/Desktop/Spring 2024/CS 510 Adv Info Retrieval/topic_modelling_bookmarker/Datasets/news_dataset_2/Data_Train.csv','/Users/laharianne/Desktop/Spring 2024/CS 510 Adv Info Retrieval/topic_modelling_bookmarker/Datasets/news_dataset_2/Data_Test.csv']\n",
    "for file in files:\n",
    "    news_categ_df = pd.read_csv(file, encoding='latin')\n",
    "    print(news_categ_df.columns)\n",
    "    for index, row in news_categ_df.iterrows():\n",
    "        try:\n",
    "            body = row['STORY']\n",
    "            line_sentences = body.strip().split('. ')\n",
    "            # Append each sentence to the list of sentences\n",
    "            variety_corpus_3.extend(line_sentences)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME',\n",
      "       'TIMESTAMP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "variety_corpus_4 = []\n",
    "news_dataset_file_3 = '/Users/laharianne/Desktop/Spring 2024/CS 510 Adv Info Retrieval/topic_modelling_bookmarker/Datasets/uci-news-aggregator.csv'\n",
    "news_categ_df = pd.read_csv(news_dataset_file_3)\n",
    "print(news_categ_df.columns)\n",
    "for index, row in news_categ_df.iterrows():\n",
    "    try:\n",
    "        title = row['TITLE']\n",
    "        variety_corpus_4.append(title)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "variety_corpus_5 = []\n",
    "folder_path = '/Users/laharianne/Desktop/Spring 2024/CS 510 Adv Info Retrieval/topic_modelling_bookmarker/Datasets/news_dataset_4'\n",
    "file_names = get_files_in_folder(folder_path)\n",
    "count = 0\n",
    "for filename in file_names:\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        for index, row in df.iterrows():  \n",
    "            content = row['description']  \n",
    "            line_sentences = content.strip().split('. ')\n",
    "            variety_corpus_5.extend(line_sentences)\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        print(e)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301271\n",
      "325232\n",
      "358339\n",
      "780758\n",
      "792821\n"
     ]
    }
   ],
   "source": [
    "variety_corpus = []\n",
    "variety_corpus.extend(variety_corpus_1)\n",
    "print(len(variety_corpus))\n",
    "variety_corpus.extend(variety_corpus_2)\n",
    "print(len(variety_corpus))\n",
    "variety_corpus.extend(variety_corpus_3)\n",
    "print(len(variety_corpus))\n",
    "variety_corpus.extend(variety_corpus_4)\n",
    "print(len(variety_corpus))\n",
    "variety_corpus.extend(variety_corpus_5)\n",
    "print(len(variety_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('corpus_lda.txt','w') as file:\n",
    "#     for sent in variety_corpus:\n",
    "#         file.write(sent)\n",
    "#         file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Sentences': variety_corpus})\n",
    "df.to_csv('variety_corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792821\n"
     ]
    }
   ],
   "source": [
    "# variety_corpus_read = []\n",
    "# corpus_read = pd.read_csv('variety_corpus.csv')\n",
    "# for index,row in corpus_read.iterrows():\n",
    "#     variety_corpus_read.append(row['Sentences'])\n",
    "# print(len(variety_corpus_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(clean_corpus,num_topics, num_words):\n",
    "    dict_ = corpora.Dictionary(clean_corpus)\n",
    "    doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=num_topics, id2word = dict_, passes=1, random_state=0, eval_every=None)\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=num_words))\n",
    "    return ldamodel,dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Preprocessing on the Corpus\n",
    "\n",
    "# stop loss words \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stop = stop.union(set(stopwords_json['en']))\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "# exclude = exclude.union(set(top_k_words))\n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "porter = PorterStemmer()\n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    stop_free = \" \".join([re.sub(pattern, '', i) for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "\n",
    "    #stemming\n",
    "    stemmed = \" \".join(porter.stem(word) for word in normalized.split())\n",
    "\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in variety_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.020*\"india\" + 0.012*\"test\" + 0.010*\"offici\" + 0.008*\"student\" + 0.007*\"updat\" + 0.006*\"report\" + 0.006*\"west\" + 0.005*\"al\" + 0.005*\"beyonc\" + 0.005*\"app\"'), (1, '0.014*\"ebola\" + 0.013*\"googl\" + 0.013*\"cent\" + 0.013*\"microsoft\" + 0.010*\"bank\" + 0.009*\"team\" + 0.008*\"iphon\" + 0.008*\"websit\" + 0.007*\"data\" + 0.007*\"featur\"'), (2, '0.021*\"year\" + 0.013*\"2014\" + 0.008*\"million\" + 0.008*\"ice\" + 0.007*\"exam\" + 0.007*\"3\" + 0.007*\"khan\" + 0.007*\"phone\" + 0.007*\"studi\" + 0.006*\"offic\"'), (3, '0.011*\"video\" + 0.010*\"galaxi\" + 0.010*\"game\" + 0.009*\"film\" + 0.008*\"star\" + 0.008*\"watch\" + 0.007*\"final\" + 0.007*\"samsung\" + 0.007*\"juli\" + 0.006*\"play\"'), (4, '0.017*\"appl\" + 0.012*\"price\" + 0.011*\"r\" + 0.011*\"share\" + 0.011*\"stock\" + 0.010*\"market\" + 0.008*\"5\" + 0.008*\"buy\" + 0.007*\"rate\" + 0.007*\"sale\"')]\n"
     ]
    }
   ],
   "source": [
    "# ldamodel_5,dict_5 = train_lda_model(clean_corpus, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.024*\"test\" + 0.021*\"microsoft\" + 0.016*\"launch\" + 0.015*\"report\" + 0.014*\"5\" + 0.014*\"rate\" + 0.013*\"studi\" + 0.010*\"health\" + 0.010*\"global\" + 0.010*\"servic\"'), (1, '0.030*\"price\" + 0.017*\"billion\" + 0.016*\"expect\" + 0.015*\"close\" + 0.015*\"featur\" + 0.013*\"deal\" + 0.013*\"dollar\" + 0.012*\"fall\" + 0.011*\"data\" + 0.011*\"airlin\"'), (2, '0.030*\"2\" + 0.029*\"sale\" + 0.022*\"big\" + 0.020*\"khan\" + 0.017*\"take\" + 0.013*\"andi\" + 0.011*\"the\" + 0.011*\"2015\" + 0.011*\"dy\" + 0.010*\"true\"'), (3, '0.027*\"galaxi\" + 0.025*\"googl\" + 0.017*\"day\" + 0.014*\"facebook\" + 0.014*\"guardian\" + 0.013*\"samsung\" + 0.012*\"beyonc\" + 0.012*\"live\" + 0.012*\"earn\" + 0.010*\"twitter\"'), (4, '0.029*\"2014\" + 0.026*\"r\" + 0.020*\"bank\" + 0.019*\"win\" + 0.019*\"team\" + 0.018*\"buy\" + 0.018*\"result\" + 0.018*\"west\" + 0.016*\"crore\" + 0.013*\"point\"'), (5, '0.051*\"india\" + 0.019*\"student\" + 0.018*\"review\" + 0.017*\"ice\" + 0.013*\"start\" + 0.011*\"univers\" + 0.011*\"intern\" + 0.010*\"car\" + 0.009*\"experi\" + 0.009*\"network\"'), (6, '0.016*\"announc\" + 0.014*\"indian\" + 0.013*\"challeng\" + 0.012*\"exam\" + 0.011*\"run\" + 0.011*\"releas\" + 0.010*\"match\" + 0.010*\"job\" + 0.010*\"2023\" + 0.010*\"million\"'), (7, '0.031*\"cent\" + 0.019*\"iphon\" + 0.018*\"3\" + 0.016*\"date\" + 0.016*\"growth\" + 0.014*\"high\" + 0.014*\"bachelorett\" + 0.012*\"hous\" + 0.011*\"unit\" + 0.011*\"risk\"'), (8, '0.038*\"year\" + 0.025*\"ebola\" + 0.016*\"news\" + 0.016*\"10\" + 0.015*\"updat\" + 0.013*\"rise\" + 0.013*\"user\" + 0.011*\"case\" + 0.010*\"offici\" + 0.010*\"viru\"'), (9, '0.022*\"appl\" + 0.018*\"video\" + 0.016*\"game\" + 0.016*\"film\" + 0.013*\"star\" + 0.013*\"stock\" + 0.013*\"watch\" + 0.013*\"final\" + 0.013*\"play\" + 0.012*\"market\"')]\n"
     ]
    }
   ],
   "source": [
    "# ldamodel_10, dict_10 = train_lda_model(clean_corpus, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.064*\"launch\" + 0.057*\"set\" + 0.043*\"call\" + 0.034*\"ceo\" + 0.031*\"fed\" + 0.030*\"june\" + 0.024*\"order\" + 0.023*\"support\" + 0.022*\"friday\" + 0.020*\"court\"'), (1, '0.042*\"facebook\" + 0.041*\"websit\" + 0.037*\"busi\" + 0.034*\"post\" + 0.031*\"australia\" + 0.028*\"trailer\" + 0.025*\"pro\" + 0.024*\"player\" + 0.024*\"medium\" + 0.023*\"research\"'), (2, '0.041*\"updat\" + 0.040*\"west\" + 0.033*\"movi\" + 0.026*\"countri\" + 0.023*\"tv\" + 0.023*\"higher\" + 0.022*\"find\" + 0.022*\"andi\" + 0.020*\"latest\" + 0.020*\"uk\"'), (3, '0.080*\"video\" + 0.042*\"ice\" + 0.040*\"big\" + 0.032*\"live\" + 0.029*\"drop\" + 0.028*\"life\" + 0.026*\"man\" + 0.024*\"woman\" + 0.021*\"list\" + 0.020*\"bad\"'), (4, '0.066*\"googl\" + 0.055*\"play\" + 0.045*\"win\" + 0.038*\"talk\" + 0.036*\"run\" + 0.030*\"actor\" + 0.029*\"product\" + 0.025*\"tech\" + 0.024*\"said\" + 0.019*\"chri\"'), (5, '0.056*\"2014\" + 0.050*\"open\" + 0.047*\"final\" + 0.036*\"10\" + 0.025*\"offic\" + 0.025*\"home\" + 0.022*\"intern\" + 0.022*\"trade\" + 0.021*\"car\" + 0.018*\"12\"'), (6, '0.048*\"2\" + 0.045*\"announc\" + 0.031*\"phone\" + 0.031*\"featur\" + 0.028*\"match\" + 0.028*\"million\" + 0.026*\"dollar\" + 0.025*\"kardashian\" + 0.024*\"recal\" + 0.024*\"6\"'), (7, '0.084*\"report\" + 0.046*\"juli\" + 0.043*\"news\" + 0.039*\"iphon\" + 0.035*\"data\" + 0.031*\"earn\" + 0.023*\"hous\" + 0.022*\"gain\" + 0.021*\"water\" + 0.021*\"group\"'), (8, '0.058*\"sale\" + 0.035*\"break\" + 0.031*\"cut\" + 0.030*\"today\" + 0.029*\"nation\" + 0.026*\"meet\" + 0.025*\"kim\" + 0.025*\"increas\" + 0.024*\"nasa\" + 0.024*\"secur\"'), (9, '0.072*\"appl\" + 0.052*\"film\" + 0.039*\"top\" + 0.036*\"star\" + 0.033*\"seri\" + 0.031*\"billion\" + 0.030*\"1\" + 0.029*\"record\" + 0.028*\"3\" + 0.024*\"amazon\"'), (10, '0.068*\"time\" + 0.051*\"student\" + 0.048*\"bank\" + 0.038*\"work\" + 0.028*\"high\" + 0.028*\"govern\" + 0.027*\"candid\" + 0.023*\"move\" + 0.020*\"africa\" + 0.019*\"class\"'), (11, '0.065*\"price\" + 0.051*\"market\" + 0.041*\"buy\" + 0.038*\"job\" + 0.034*\"rise\" + 0.026*\"4\" + 0.024*\"airlin\" + 0.024*\"ahead\" + 0.024*\"oil\" + 0.022*\"end\"'), (12, '0.058*\"year\" + 0.052*\"show\" + 0.039*\"the\" + 0.030*\"month\" + 0.029*\"date\" + 0.025*\"emmi\" + 0.024*\"face\" + 0.023*\"viru\" + 0.021*\"found\" + 0.020*\"death\"'), (13, '0.066*\"r\" + 0.045*\"result\" + 0.044*\"deal\" + 0.041*\"reveal\" + 0.036*\"app\" + 0.031*\"bachelorett\" + 0.030*\"univers\" + 0.029*\"american\" + 0.029*\"gener\" + 0.021*\"outbreak\"'), (14, '0.071*\"share\" + 0.051*\"back\" + 0.043*\"hit\" + 0.039*\"al\" + 0.036*\"growth\" + 0.036*\"point\" + 0.023*\"septemb\" + 0.021*\"interest\" + 0.020*\"head\" + 0.020*\"mark\"'), (15, '0.046*\"offici\" + 0.039*\"challeng\" + 0.032*\"review\" + 0.031*\"award\" + 0.031*\"offer\" + 0.029*\"china\" + 0.025*\"case\" + 0.024*\"fall\" + 0.024*\"state\" + 0.023*\"take\"'), (16, '0.062*\"watch\" + 0.047*\"rate\" + 0.045*\"plan\" + 0.038*\"close\" + 0.031*\"part\" + 0.025*\"continu\" + 0.024*\"risk\" + 0.023*\"devic\" + 0.022*\"cast\" + 0.022*\"manag\"'), (17, '0.065*\"world\" + 0.063*\"game\" + 0.052*\"stock\" + 0.040*\"team\" + 0.037*\"studi\" + 0.034*\"guardian\" + 0.028*\"global\" + 0.026*\"music\" + 0.024*\"cup\" + 0.023*\"fan\"'), (18, '0.044*\"compani\" + 0.039*\"5\" + 0.032*\"profit\" + 0.031*\"user\" + 0.028*\"health\" + 0.026*\"onlin\" + 0.024*\"twitter\" + 0.023*\"issu\" + 0.020*\"board\" + 0.019*\"network\"'), (19, '0.074*\"test\" + 0.066*\"day\" + 0.037*\"expect\" + 0.033*\"season\" + 0.031*\"start\" + 0.026*\"releas\" + 0.025*\"return\" + 0.024*\"director\" + 0.021*\"full\" + 0.019*\"gold\"')]\n"
     ]
    }
   ],
   "source": [
    "ldamodel_20, dict_20 = train_lda_model(clean_corpus, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # business \n",
    "data1 = extract_text_from_url(\"https://www.nbcnews.com/business\") \n",
    "#lifestyle and entertainment\n",
    "data2 = extract_text_from_url(\"https://www.hollywoodreporter.com/movies/movie-news/dermot-mulroney-didnt-work-year-after-my-best-friends-wedding-1235880937/\") \n",
    "#technology\n",
    "data3 = extract_text_from_url(\"https://www.cnbc.com/2024/04/25/nvidia-backed-synthesia-unveils-ai-avatars-that-can-be-generated-from-text.html\") \n",
    "# politics\n",
    "data4 = extract_text_from_url(\"https://www.nbcnews.com/politics\")\n",
    "#movies\n",
    "data5 = extract_text_from_url(\"https://www.badmovies.org/\")\n",
    "#literature\n",
    "data6 = extract_text_from_url(\"https://en.wikipedia.org/wiki/Shakespeare:_The_World_as_Stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_20.save('lda_model_20.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
